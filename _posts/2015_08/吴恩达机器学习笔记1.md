title: 吴恩达机器学习笔记1
date: 2015-08-23 09:40:00
updated: 2015-08-23 09:40:03
tags:
- 算法
- 机器学习

layout:
comments:
categories:
permalink:

---

* **[监督学习](#监督学习)**
* **[线性回归问题](#线性回归问题)**
* **[梯度下降LMS算法](#梯度下降LMS算法)**
* **[线性回归问题通用计算方法](#线性回归问题通用计算方法)**
* **[局部加权线性回归](#局部加权线性回归)**


# 监督学习

设输入变量$X$为n维向量，表示n个特征，$y$为输出结果，为实数，是我们需要预测的结果,
$\{ X_{i},y_{i} \};i = [1,...,m],X_{i} \epsilon R^{n}$
为m组输入学习样本，也叫训练集。
目标函数为$h_{\theta}(X)$为我们的预测值
当$y$值取值为无限集合时，我们称这类问题为**回归问题**， 当$y$值取值为有限集合，我们称这类问题为**分类问题**

# 线性回归问题
假设$X \epsilon R^{n}$，为$n$维向量，我们设$h_{\theta}(X)$函数为线性函数：
$h_{\theta}(X) = \theta_{0} + \theta_{1}x_{1} + \theta_{2}x_{2} + ... + \theta_{n}x_{n}$
简化为：
$h_{\theta}(X) = \sum_{i = 0}^{n} \theta_{i}x_{i} = \theta^{T}X$
对任意输入样本$X_{i}$，我们用$\theta^{T}X_{i}$预测$y_{i}$。我们设置一组初始向量$\Theta$，同时定义如下函数：
$J(\theta) = \frac{1}{2}\sum_{i=1}^{m}(h_{\theta}(X_{i}) - y_{i})^{2}$
对于m组输入样本，我们要求在$J(\theta)$最小时，$\theta$的值

### 梯度下降LMS算法
机器学习的表就是找到向量$\theta$，使$J(\theta)$取值最小，我们使用如下搜索算法：
设置$\theta$初始值，然后不断的更新$\theta$使$J(\theta)$更小：
$\theta_{j} := \theta_{j} - \alpha \frac{\delta J(\theta)}{\delta \theta_{}j}$
其中$j = 0,...,n$，$\alpha$为学习速度
$\frac{\delta J(\theta)}{\delta \theta_{}j}$为$J(\theta)$的偏导数
通过不断的更新$\theta$，使其从初始点，按照最快的梯度向下更新\theta，直到一个局部最优点（局部最优点不一定是最终最优点）
先假设只有一组输入数据${X,y}$，所以公式可以简化为：
$\frac{\delta J(\theta)}{\delta \theta _{j}} = \frac{1}{2} \frac{ \delta (h_{\theta}(x) - y)^{2}}{\delta \theta_{j}} $
$     = 2 \cdot \frac{1}{2} (h_{\theta}(x) - y) \cdot \frac{ \delta (h_{\theta}(x) -y)}{ \delta \theta _{j}}$

$     = (h_{\theta}(x) - y) \cdot \frac{ \delta (\sum_{j=0}^{n}\theta_{j} x_{j} -y)}{\theta _{j}} $
$     = (h_{\theta}(x) - y) \cdot x_{j}$
所以更新动作简化为：
$\theta_{j} := \theta_{j} + (y - h_{\theta}(x)) \cdot x_{j}$
从更新动作可以看出，如果预测值与样本值相差太大，调整步长就大，反之，就小，**快速收敛**，但到达最低点附近后一直徘徊

# 线性回归问题通用计算方法
先定义矩阵的偏导数：
![](/images/ml1.png)
定义输入m组n维特征数据：
![](/images/ml2.png)
m个目标值向量：
![](/images/ml3.png)

设$X\theta - y$为：
![](/images/ml4.png)
$X\theta - y$的长度为：
![](/images/ml5.png)

对$J_{\theta}$求导得：
![](/images/ml6.png)

令求导结果为$O$，最优点，得到求解$\theta$更一般的公式：
![](/images/ml7.png)
根据上面公式，给出m组训练样本，我们可以一次性求解$h_{\theta}(x)$函数

# 使用概率论解释线性回归求解方法的合理性

# 局部加权线性回归
当输入n维数据$x$时，要求$h(x)$，以前的算法：
* 求解$\theta$使![](/images/ml8.png)最小（只需要在输入m组样本时，训练求解，后续不用再求解）
* 输出$\theta ^{T} X $
局部加权线性回归算法是这样：
* 求解$\theta$使![](/images/ml9.png)最小（注意，看后面$\omega^{(i)}$函数定义可以确定，每次预测都需要算一下$\theta$）
* 输出$\theta ^{T} X $
其中$\omega^{(i)}$函数定义：
![](/images/ml10.png)
当x越接近训练样本$x^{i}$时，$\omega^{(i)}$越趋近1，反之，$\omega^{(i)}$越趋近于0

局部加权线性回归算法属于：非参数化模型中的一种，即：不训练任何参数，而是每次预测时，使用输入数据进行预测(每次都要计算参数)

# 逻辑回归
逻辑回归问题中，$y \epsilon {0,1}$，我们重新设计$h_{\theta}(x)$：
![](/images/ml11.png)
其中$g(x)$函数图形如下：
![](/images/ml12.png)
从图中可以看出，$(g(x))$取值在(0,1)之间
其中$\theta ^{T} X $展开为![](/images/ml13.png)